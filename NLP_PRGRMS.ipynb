{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVSqBRDoLn5A3ELluhiSao",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhxrg/NLP/blob/main/NLP_PRGRMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4EDreNYk6zh"
      },
      "outputs": [],
      "source": [
        "#nlp prgrms\n",
        "# w/t packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Study of Python and basic commands to access text data. (from notepad, pdf, word documents,online)"
      ],
      "metadata": {
        "id": "edn7JUHUk-rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a text file and read its contents\n",
        "with open('/content/txt_file.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "    print(content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYt31LlGlJHV",
        "outputId": "4b9e4f85-a46a-4c59-90a1-8254883d3fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi I am Bhargavi, I like eat, sleeping and walking and running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your PDF file\n",
        "pdf_path = '/content/pdf.pdf'\n",
        "\n",
        "# Open the PDF file in binary mode\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    # Read the raw binary content\n",
        "    content = file.read()\n",
        "\n",
        "    # Print the raw content (for illustration purposes)\n",
        "    print(content[:100])  # Print the first 100 bytes as an example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vokEVZ-AlmL3",
        "outputId": "845b0a39-7132-4104-a17f-e0f1b0641678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"%PDF-1.7\\n\\n4 0 obj\\n<<\\n/Filter /FlateDecode\\n/Length 28572\\n>>\\nstream\\nx\\x9c\\xed}Kv\\xe3J\\xcf\\xe4\\xbcV\\xc1\\r\\xd0'I&_\\xcb\\xe85\\xdcs\\xba\\xff\\x81j\\xd0\\xbd\\xffA\\xcb\\x12\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a Word document and read its raw binary contents\n",
        "with open('/content/cop.docx', 'rb') as file:\n",
        "    content = file.read()\n",
        "    # Convert bytes to string (assuming the text is ASCII encoded)\n",
        "    text = content.decode('ascii', errors='ignore')\n",
        "    print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayQUESD7ltql",
        "outputId": "3fe4a7d3-2f7d-4006-ba25-e0a6e7d76a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PK\u0003\u0004\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005\u0000\u0000\u0000word/PK\u0003\u0004\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000b\u0000\u0000\u0000word/_rels/PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX\u0013\u0000\u0000\u0000\u0019\u0003\u0000\u0000\u001c\u0000\u0000\u0000word/_rels/document.xml.relsMn\u00021\f\u0012ydR!D`*\u0001B#&q\u0018\u0004oP\u000b\f\u0012\u001aY\u0015'\f!` \u0019*\u001aW)}^-NQ+.*Bhj:fNIjNe+<\u0001b[(\bb\fbw7ecpCh\b\u0019bL:T\n",
            ",|\u001d?\u00192\u001d\u001eC.AL(\u0011wp >\u0006]\u00042GwW!D0d\u000eM!?\u0005PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX&Q\u0004\u0000\u0000\u001d\u0014\u0000\u0000\u0011\u0000\u0000\u0000word/document.xmlKr8\u0010s\n",
            "Y\u001ep,ga\u0013W%)W9\u0000D4I@\u0000\u00059g\u0011*9`0\r>$r\\zdcoDD\u0006~4<p\u000b\u0004c`\u001f\u0004\u0004d\u0018,vwu2\ruT2*YP\r>\u0011Sqt\u0004\u0001FgA\u0019s\u001e\u001beUC$<TpPF`-vA4AD:l4Y\u0014\u0004:>\n",
            "IQ02j\u0011'+I8^:\u000be;\u0003\u0001>(i3a\u001cJY\u0007Y>\u00172\u0017j\to[KCK\u001a\\4?O\u001c\u000evX\u0011XY1;Orzfcr\u0003\u001e\u0003tz|4k\u001a?v-\u0017+{E\f\u001emFj\u0007Zy0qpf\f\u000bm\u0000\u0010\u00068\u001an{&j\u000b\u001f-~Lz\"a60x4=%'L$\u0003P|h>f{\u0013N\\1\u0010u2\u001cNbq{t{mn0^9|G:d\u001c\u00062(.7:Qm\u0012Kg_;\\9:@\u0012\u001c\u0004?k\u000bb[W\t eb\u0016|\u0002Q\u0010>O].EV\u0018\u0005Bi.]mc\u001a8\r\u0005=\u0018?\u0007E;Tgq(eQ!T+%J\u0002ITaN\u0004%2\u0015\rT\u0011'{VF\\,Eozv{\u0006o^/lx|e@~P$Ah:\u0004~l(8:\u001eY\u0011*\u00120\u001f[\\;K4M2V\u0006\u0018\u0010\u001b*!B-G\f1\u0002LO8\u0003\u0016\u0006T\u000f_\n",
            "A1#/U\n",
            "$\u0003S!) 9\u0006M\u0002Lh\n",
            "\\d\u001d*+\u0006V\u0003)1@piV\u001evZeLq!Tb\u0013~LA1\u0003u!\u0017$WV\u001eZP)Q!~}\u000b8^\u0001j\u0003cj\u0003<\u0018RpgA$NHA9K\u001d&e0\u0014\u0015\u0015f!8`U\u0013\u001aV`Z<*){\t\u0013pP KpmMIi<R k\td>U9\u001cq\u0005lTbw\"p%AI0\u0015U 9\bQ/+\u0000A\u0012\u0003E9B\u0003\u000f`~|<\u0015\u001eUkz\u0017:STFX\u000e',\re\u001cQNPOq..tCW9V!*_5\u0005$g1A=DhnE~Wi\bt\u001f\r\u001bu7x2h'\u000b+pePK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX5u{A\u0011\u0000\u0000\u0000\u0000\u000f\u0000\u0000\u0000word/styles.xml][w\u0011+<zj\u001f\u001co>c;q6zcg\u0019\"!kPI*\u000b\u0017Q\u001acIs\u0016\u000073\u0000 o|\u0015Yx`\u000b\r:_Nf\u001d+Y0O\u0004ox\u001aO\u001e,)\u001eD}Y\u0015YFy o.EX\f>\u000b\u001f6\u0013[k\u0016\u000bstR\u000eX.]\u0015A!$\b2:h\"ty\u0014&^37a#\u0000v(\"5PR\u0016`\u0003\u0018\u00000f)FWJ\u0016q\\\u00073q\\2\u0005\u0015\n",
            "edYV,Z\u0015\u00119R\u001cW:\bHuK\u0012gi`+aeM(_p/#3\t/+\u0011x\"ud\u0005e)+\u000b>\u000f\"#\u001fw8ek\u001d;\u0017vI_w\u0019+5\u00055\u001e|-\u0016yF]ZH\u000e\u000bOn`7m[w_5]]\u000e[\\zzS`:~|(M,Bi!E.P\f\u00002\u001c&QI\u000f~m,outY7+B\u0019y:yz8<(<\u0018\\;\\^&\u000f\u0013m\b^fEn\u0014'3~D3\u0015>\u001c\r1(\n",
            "O\n",
            "k{\u001f]X\u0005UX\u0005MUX\u0005U\u000br\u0003G~MC8\u0003\"!\u0011\bgL3!\u0012pZ\u0019'\u00166\u000e4 \u0005]\u0002\u0011.\u0001pA'\u0005\b\u0017s\"\\\u0010pA&MZ{fA\u001am)D\u001c[1\u001e\u0005\u0012Kg4x!I#\t`vl\u0003\u000bcYbi-M\u0015W5H<B0/y\u0003S\u001a6\u001d\u0004`/\blsx\u0010/C$\t\n",
            "Ay%0j ,>|pR 8\u0011'\u001a\u0013Xs\u0003\r>503\u0003\r>1(pF\u0014HS)\u001aR4\"%I\u0014Ho)\u001aRzsc\u000fC.=\u0011Q\u0004[>`r\u0000IL\u001b\u0016W\u0006Ml]p;>-G\u001ak\u0013vM{Q9WG^9\u001ex]\u001c&\u00015M>sYN[?+e&\u00196\u0016\u0003\\aD\u0006\u0004\u0016I\rgzZ\u0016[G%\u0004@\u0013<iCk+y;tq(\u0012[+`P\u0015\b@\u001d\tGn\u001b\u0001\roN|z\u0016\b\u0007NU\u0014C\u0003x!XdL?T\\&\u0013Qk4K$H!BL7pIP\u001f\u0010,thnBl\u00029\u0011-\u001eo(\u000fhHBW\u000bQ9\u001d\tXa0,vPIX$3Cob=S\u001d\u001dC\u0004\u001e\u00124v\u0007}cw\u001a{(rmfxT>K'S`\u0006H\fLADbG`G^Bx\u0004Sr\u001a_`T4h0*\u000e4\u0018)\u0001w\u0014o)\u0011\r\u0001\n",
            "`TvF\u0013\u0014LQ\u00063\rFeg\u0016_. )@R\\\u0001\tbE'\"w\u001eg\u0004\u0013\tM(\u0010\u0011$)EL9NH/(E0#<O\bm%w7\u001d\u0012/sl\u0012CC*_,bv\u0017a&Y#v2O\u0006La}aEv2:,\u001dIHkJ2'%\u001diMIX$;,|(5i9\u000bpiUKv\\:mZ\u00003fzcx\u0019\u0005Nf~er\u001bQ\u001fXwO?\u001d9\u0018,S\u000f^\u000e[8\u000bW;Q\fQ;!j\u0007 3DHd\u0014G$3Jd\u001d\u0010h\u0005{\u0004\\h\u0005D+$Z\u0018\u0005!j\u000f\u0007\u0010hG\u0010hGm1R0C\u001c\u00157rTvT\bvT\bvT8\u00009*9*o\u0010B\u0014B\bB\bB\bB\b6\u001c\u001b\u001b9*DA;*@;*@;*x}\u0011P\u001ePB&\n",
            "Q\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!P\n",
            "\u001b9*DA;*@;*@;*x\u001f\u0018P\u001ePB&\n",
            "Q\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!P\n",
            "\u001b9*DA;*@;*@;*x\u001eP\u001ePB&\n",
            "Q\n",
            "!\n",
            "!\n",
            "!\n",
            "!\n",
            "!P\n",
            "\u001b9*DA;*@;*t4GTsU9PYXE\u0010*}p8\u000f.<W)'\u0000C\u0003\u001b>>Q)\u0010z\u0014LaJ9Q\u0017%A7$\u0018uoQ\u0012t2\"# \\\u0015f\n",
            "}xU.C\u0015W pUd.\bB\u0005WRy_z\\SO|)@2\u0002Pe+~m\bu3#\b'\ff\fjfX;\u0019\u0001K5DhD5iN5jL5jF5\fX!\u0002j\u0000j\bj\bja\bX!\u0002\u001d\u00119\u00101\u0010\u0019pp\u001a\"`\bX!B#\u0001Ls!Tc!T3A\u001a\"`\bX!B#\u0001Ls!Tc!T\u0015z\u0016yT\u0010\r\n",
            "\u000e \u000b\u0005\u0006RAaT@h-AeKEeKEeKE\u001aeKfR)ReKfqR\u0019\u001dYTF5.[2R*eKT%3lj\\TFu,[2R*eKT%3lj\\TF5.[*e,[\u001a-URT2qR\u0019lj\\d\u001a-UR*eKfqR\u0019lj\\TF5.[2R*eKTR%\u0004^5V1kq/A#}E\u000fVv\u001fwRx>|,u^x]Iz\u0014\u000fr*8%jb\u0007\u000f\n",
            "I\u001anQZ1\u001enKX0\u0003a Y1+\u0016&w=3oK~D\u0003OnC#k{zB}9Ls\u001c_\u001ef6K}S$f\u0004eT\u00066\u0006sg48u48ug{\u001a|9h:&\u001fV{LqqQ&00\u0016\u00065laH_<?y\f\u0007<k-ch4!y\f_y\fT\u000ffZKGFGT^Y0&\u0019xoft\t%pTHN99w:^|Bksr\u0004|>\u0006gF\u0006g\f!GTc-HgndpNu18;bvS/99\u0015%u\fLs\u001e9rb$N\u001db^'Tj(\u0012V\u001f\u0003G\u0002<G'5uN%,yZz|\u0019'wYErbQ>SFney#N\u0010N5NeTwM$u\tjyY\b\u0017J\u0014\u0007\u000f7f6\u0001\u0001\u0011C6\u0013R\f\r\f\r\u0018\u001a\"\u0019a*=Bj{Dl2Z\u001d#:&pK=Nz\u0010q*T\u0014)R/a\u0019R3\"5^6!\t+uNK3Q^SK7%\u001fnOtN:=0ovt\t\u001co[-&*dJj\u00012Jf\u0014SrF\u001beIl\u0013XVmO?7[%ze~8\u0016/\u0002bcLreM;L\u001fP{\u000b\u0006\u001b?-<3o~%\u0000>}\u000b58\fJI5}4Rjf\r\u0018Mz\u001d\u0004UbSL*yRFV\u0007Z_o\u0016\u001b'3y4YO\u0018}[\u001b+oRWcGN\r+uAbz\u0003J)SXOG\"*\n",
            "MWgvPAvu=w\u0017\u001cN4^kJ~s\u0004'L}mM\\\u0004;m|$\rdk*@? ir7\u000b\"ji\u001b\\\u0016\u0011k\u001e{gx}\u0014\u000e\u0007\u000b};&]\u0001\n",
            "GEPF/a1Z5\"#, \\'gwu1\u000f\u0005H#ddt\u001b\u0000J3auj_H\u0004\u001eF\u0007_Y>JV\u001bJ\u001f5L\u001d~\u0016\u0000]@\u001d\u0002FQ\u0015t\b\u001a$s\u001f\u001dH,7btm-]Zwe\u0018\b\u0015YG\u0002e]u|\u0016>l'5 \"v?C\u001f\u0006tmy\u0005=7~\u001e\u00133?A=//\u0007{|\u001d@=\n",
            "\u001eP\u0017\u0014`\u0011Nhk7a.t(~A\u0019Kn~UW&\u0018;bt\u001c>\u0016\u0007\u0007WjU>=N\u0000%%'atz^\u001eE\u000e42pR{1X^8bW(%h\u0017\u001dPUU0\u00134n\rW`8Dn\b-Fo\u0001~v.mEgMd>}y1dbwbu<\u0019^f;\u0016\u00155w.>QRt%\"\u0014\u0017w\u001ei$\t\b^\n",
            "\u0007.\u0004GY\u001e-b+[\u0001PK\u0003\u0004\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\t\u0000\u0000\u0000docProps/PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yXt&LE\u0001\u0000\u0000\u0002\u0000\u0000\u0011\u0000\u0000\u0000docProps/core.xml]k0\u0014J}6J\u001ba\u0013&\fl.$G\rk>Hj]&\u001eTyitH\u000447BMVy\u0012\u001f\u00161\u001ajt\u00043\u0016\\}m!\u0012c:Bw8Bwt\u001bl\u0019f\u001bE?`\u0005\t\u0016\u0018\u000eFtV\n",
            ">(5@p\f\r(c\u0011\u0006pOH%]\u0012\u000e\u0001l6kG='sJ\u001d7\u0001J2\u0000\u0019=+<\\;`8f\n",
            "D\u001f_.v\u001b{\u0004|f\u0011w\t)X7rrHIc2[\u0011-biNdRQ9&Y1)b\u001bTKz_E\u0000PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX7\u0002\u0000\u0000\u0010\u0000\u0000\u0012\u0000\u0000\u0000word/numbering.xmlXKn0\u0010@,*D\t\u0006)\\\u00074=\u0000-6\u0011~\u00043t]z%J2\fh3=3c\u001bgNNTJ\b\u0001!\"1\u0015\u0010|}z\u0019\u0003'XIAB\")]\u0006\"Sc\u0010D\u001a,(\u0004\u000b\u00004Z\u0010\u001e^$9\u0019\b\\J\u0015CEnWdD\")c4\u0010a38S!9Kn\fz5RF`\u0012F S\"@TlH&Y\bU':AF\u0019'B\u0017\u0019\"p\"]d+-1.J3P]\u0001/\u0007f\u0002\u001f8[3?\u001a7b!:\u0014sL8b\u001c.\u001a4\u0003\u000e\u0001eV,&\u0000ks|YT \\H02WSw\u0006i\u0015;{_.`\u0003ELRvs^4Q\u0015!p\u000b\u00141MUB\fPa*\u001a6fm\u0000Z_3@b\u0004Wuif\u0011]!>o\u001fedqO>+P\u0011\u001b\u000eL\u0005\u0016I\u001bgX`\u001dG!)y-{WawS\u001e\u001a`\u0016\u001cC\u0005AG^oSjG]a?jS\u001f\u0016MK2&\u0011<\u0017w\u0000\u000b\u0003k\u000eF&B\u00033~s\u0001GS\u0013G5\u0004\u001fg\u0005-\u0004\u001c\u000eNoAs\u0001N\n",
            "8n\"h\u001e}W&\u000e)'\u0006\t*;m\u0014\u000b+Z0PK\u0003\u0004\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0006\u0000\u0000\u0000_rels/PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX\u001f\u0000\u0000\u0000\u0002\u0000\u0000\u000b\u0000\u0000\u0000_rels/.relsJ\u00031\u0010_%;VDi/RM>@Hfw\u001f&Soo(VC7C\u0016CWS0mZP\u0014mr>\u001a\u001d'\u00192\\TmE \u0011\u001d(Ltz\u001e/'-O\u00062ig[zHv\u001f('~%*pO-CYn*\u0016r'@b\u001161M2n\u0016O[<r9&\\\u000f\u001d#7dr\u001e3\u0017I\u0015\u001d3_Jx1\u001fPK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX?k\u0001\u0000\u0000\u0006\u0000\u0000\u0013\u0000\u0000\u0000[Content_Types].xmlUn0\u0010(*1*\u0002>-\u0007\u0001\u0004Cw@ Jn^R\r8/t\u0012G.c<t#jkD\u0017kP&0NqrK`w=\u0013F#hL0gg(x]aY,U[]8f*>-%$+e{P(eSzHHD&L\u001dT\r7D\u0019\u0010r8\u0016t\u0004NpQ4S\u0014R@nDH~\u001b[g\u0004xOC\u0014\u000e,YG\u00193]\u0005t\u0016aB8HCy\u0002.\b,\u0010hy\r\u0010v),{j=uV|z\u001d<j\u0018\u00066y@Q#\b\u00020\u0010\u001e\"/3\u0001PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yXXy\"\u0000\u0000\u0000\u0000\u0000\u0000\u0013\u0000\u0000\u0000docProps/custom.xmlA\n",
            "0\u0010T\u0017\"i7Eu\u001fi\u001bhfB&-F\u0004\u000fk_\r8&\r\u0002ho\u000b\u0014\f\rfaB\r;\n",
            "tms\u001c0&RdDR\u0012;7RL\u001cIyI8:WGJTUgeWI\u000f/9g\rPK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000docProps/app.xmlA\n",
            "0\u0010m\u000b\u001bq\u000f\r43!\u0013K{{#\u0007p\u0016\u0016BbO\u0000<NZ>\"\u0005g,\u000e,'\u0007\u0016\u0005@r96J!\u0018JRFJ2h\u001c+W\u0000Tg\u0005[\u0006t\u000e\u0007_\rPK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX\u0001\u0000\u0000\u0006\u0000\u0000\u0012\u0000\u0000\u0000word/footnotes.xmln \u0010_`Gjeaz\u0007\u0004 wl\u0013VQz1?a}G\u0013K0\u0015)9f_\u001e\u0016f5@0\u0010P`|[^&\u0004[Ry#4K-\u0003\u000fuXr\u0014ZrA{p;||\u000e\u001e?'\u0011\u0015\u0006\u0017kp\u0005=vt|J\u0003oG\fTuE24HP\u001c\n",
            "wIIr\u000f1#uB\u00070v.4\\l\\\u0011V$\u00078K&V\"#\u0003\").o\u0013\u0013jkN6\u0018`\u0016`54y\u001d&\u0011\u001fb&3m\u0013y7BG\fw=\u001b~krzd}\u0019\u000e\u0016#\u0000'\u0018\u0003xr-\u0001\u0003X\u001d\u0004|\bVryu&\u0001\biO\r\u00075dCt,j\u0014Q\u0018y9~Od;-3U\u0004i-=\n",
            "[]8_PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yXwm\u0000\u0000\u0000{\u0000\u0000\u0000\u001d\u0000\u0000\u0000word/_rels/footnotes.xml.relsMA\u000e\u0002!\fEBw.1n\u000e`\u0000\rV \u000ePb<,]~n>4\u0015qp,\u0018\u0016_I};\\`]\u001b1U5#\u0011u\u0010{WD3T* 21[JMd\u0019\u0007\u0003PK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX(\u0001\u0000\u0000s\u0005\u0000\u0000\u0011\u0000\u0000\u0000word/settings.xmln \u0010_\u001d5UeFmszH\u0000\u00136Z\u0018\u0010`\u001duJn\u00043c5YB\u000e+Vls)\u0014Njl*'\u0004(8T\u0015no2P\b\u001cX/9U\u0016j\u0011\\tu\n",
            "gk-\u0014\u001f\\*/i\u0013*F\u0002}\u0005!\u0003Os^!\u0005k\u0017,$\f\r\u0010v\u0010CO'vqu\u0001\u0003bs44JaY\u0014*LSF\u001e!\u000f\u000ecz(.Kc\u000b\u000f>@\n",
            "<E&\u0016\t\u001d\u0019\u0011G)\u0016^\\X&~Wi\u0015>\u000fp\u001a=4}\u0019\u0001wG`\u001dj23-xzV\u000f\r\u0000O\u001cQ2z6l8RGo`\u0005Zd|\f^g@4\u0007S\u001aLTl:3Ou8\u000fdqh3:\n",
            "\u0010,y}1~:9kNPK\u0003\u0004\n",
            "\u0000\u0000\u0000\b\u0000yX9\u0001\u0000\u0000\b\u0000\u0000\u0011\u0000\u0000\u0000word/comments.xmlr \u0018\u0006[q8WXS7'x\u000b4\f_R%ING$I4#5+Y|\u0005X\u0011.5xuH\u0012(Ikp\u0016<=>\u0015VBP\u0003VT\u0003 QJplU;BqL!16,\u000eb'\u001bld\u0003m\f\u0015\tP\"l]\bK|HIY\\&\u0015t&ci&E\u00114N\u0019iP yK\u000fk\u0007o;{3+\u0003LHzAl\u001e\n",
            "Eh&AQ58\u0018Y]|\u0017_?n;\u001c\u001a.k, \u0016q\u0014M*Co(L\u001d>_\u001c)g1&#\u001dOL!ojF\u0013\u000f\u0000\u0014\u0011Pb:\u000fj@<\u0013Fpd\u0019\u00018f)E\u0015v!,\u001bt^M#m#\u0018uo^c\u0016ma0)\u0001PK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000word/PK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000b\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000#\u0000\u0000\u0000word/_rels/PK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX\u0013\u0000\u0000\u0000\u0019\u0003\u0000\u0000\u001c\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000L\u0000\u0000\u0000word/_rels/document.xml.relsPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX&Q\u0004\u0000\u0000\u001d\u0014\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000f\u0001\u0000\u0000word/document.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX5u{A\u0011\u0000\u0000\u0000\u0000\u000f\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000S\u0006\u0000\u0000word/styles.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\t\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0017\u0000\u0000docProps/PK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yXt&LE\u0001\u0000\u0000\u0002\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0017\u0000\u0000docProps/core.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX7\u0002\u0000\u0000\u0010\u0000\u0000\u0012\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\\\u0019\u0000\u0000word/numbering.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\u0000\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0006\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000^\u001c\u0000\u0000_rels/PK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX\u001f\u0000\u0000\u0000\u0002\u0000\u0000\u000b\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001c\u0000\u0000_rels/.relsPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX?k\u0001\u0000\u0000\u0006\u0000\u0000\u0013\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001d\u0000\u0000[Content_Types].xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yXXy\"\u0000\u0000\u0000\u0000\u0000\u0000\u0013\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000-\u001f\u0000\u0000docProps/custom.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX\u0000\u0000\u0000\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001f\u0000\u0000docProps/app.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX\u0001\u0000\u0000\u0006\u0000\u0000\u0012\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000word/footnotes.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yXwm\u0000\u0000\u0000{\u0000\u0000\u0000\u001d\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\"\u0000\u0000word/_rels/footnotes.xml.relsPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX(\u0001\u0000\u0000s\u0005\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000K#\u0000\u0000word/settings.xmlPK\u0001\u0002\u0014\u0000\n",
            "\u0000\u0000\u0000\b\u0000yX9\u0001\u0000\u0000\b\u0000\u0000\u0011\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u001a%\u0000\u0000word/comments.xmlPK\u0005\u0006\u0000\u0000\u0000\u0000\u0011\u0000\u0011\u0000\u001e\u0004\u0000\u0000\u000e'\u0000\u0000\u0000\u0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Perform text pre - processing on a given corpus without using any pre -defined NLP packages.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Sample data creation (replace this with your actual data loading logic)\n",
        "data = {'content': [\"The weather was beautiful. I went for a walk in the park. It was a sunny day, and the birds were chirping happily. Suddenly, a black cat crossed my path. I stopped and watched it disappear into the bushes. After that, I continued my stroll, enjoying the tranquility of nature.\"]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    stop_words = set([\"is\", \"an\", \"the\", \"this\", \"another\", \"i\", \"it\", \"was\", \"in\", \"for\", \"a\", \"and\", \"of\", \"my\"]) # Add more as needed\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming (using a simple example)\n",
        "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
        "    # Lemmatization (using a simple example)\n",
        "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the cleaning function to the DataFrame\n",
        "df['cleaned_content'] = df['content'].apply(clean_text)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "ExYkXEU_l8DW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a451fc9-a163-4113-c31e-dd115d02ea7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             content  \\\n",
            "0  The weather was beautiful. I went for a walk i...   \n",
            "\n",
            "                                     cleaned_content  \n",
            "0  weather beautiful went walk park sunny day bir...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.)Implement N -Gram model in python without using any predefined NLPpackages. Note: use corpus of your own choice.\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data creation\n",
        "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to generate n-grams\n",
        "def generate_ngrams(text, n):\n",
        "    tokens = text.split()\n",
        "    ngrams_list = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "    return ngrams_list\n",
        "\n",
        "# Apply the function to generate bigrams\n",
        "df['bigrams'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
        "\n",
        "# Print the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVnBY71mrRko",
        "outputId": "cc101092-63de-47dd-cc2d-b5004d870ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               cleaned_content                                         bigrams\n",
            "0  this is an example sentence  [this is, is an, an example, example sentence]\n",
            "1     another example sentence             [another example, example sentence]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Implement Part-of-Speech (POS) Tagging.\n",
        "# \"The cat chased the mouse around the house. Birds sang in the trees while the\n",
        "# sun shone brightly in the sky. A group of children played happily in the park,\n",
        "# laughing and running around.â€\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data creation with provided text\n",
        "data = {'cleaned_content': [\n",
        "    \"The cat chased the mouse around the house. Birds sang in the trees while the sun shone brightly in the sky. A group of children played happily in the park, laughing and running around.\"\n",
        "]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to generate simple POS tags (Noun, Verb, Adverb)\n",
        "def simple_pos_tagging(text):\n",
        "    tokens = text.split()\n",
        "    pos_tags = []\n",
        "    for token in tokens:\n",
        "        if token.endswith('ing'):\n",
        "            pos_tags.append((token, 'Verb'))\n",
        "        elif token.endswith('ly'):\n",
        "            pos_tags.append((token, 'Adverb'))\n",
        "        else:\n",
        "            pos_tags.append((token, 'Noun'))\n",
        "    return pos_tags\n",
        "\n",
        "# Apply the function to generate POS tags\n",
        "df['pos_tags'] = df['cleaned_content'].apply(simple_pos_tagging)\n",
        "\n",
        "# Print the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONNWqNJ6wUhd",
        "outputId": "0ef17f16-7fb0-4c0b-bdb0-c1b0e1dee667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     cleaned_content  \\\n",
            "0  The cat chased the mouse around the house. Bir...   \n",
            "\n",
            "                                            pos_tags  \n",
            "0  [(The, Noun), (cat, Noun), (chased, Noun), (th...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Implement chunking to extract Noun Phrases.\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data creation\n",
        "data = {\n",
        "    'POS_tags': [\n",
        "        [('this', 'Noun'), ('is', 'Noun'), ('an', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')],\n",
        "        [('another', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')]\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to perform simple noun phrase chunking\n",
        "def simple_noun_phrase_chunking(pos_tags):\n",
        "    noun_phrases = []\n",
        "    current_phrase = []\n",
        "    for token, tag in pos_tags:\n",
        "        if tag in ['Noun', 'Adjective']:\n",
        "            current_phrase.append(token)\n",
        "        elif current_phrase:\n",
        "            noun_phrases.append(' '.join(current_phrase))\n",
        "            current_phrase = []\n",
        "    if current_phrase:\n",
        "        noun_phrases.append(' '.join(current_phrase))\n",
        "    return noun_phrases\n",
        "\n",
        "# Apply the function to generate noun phrases\n",
        "df['noun_phrases'] = df['POS_tags'].apply(simple_noun_phrase_chunking)\n",
        "\n",
        "# Print the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b669Q4M0wyWo",
        "outputId": "055d2aca-01c0-4633-f09e-28627d399f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            POS_tags  \\\n",
            "0  [(this, Noun), (is, Noun), (an, Noun), (exampl...   \n",
            "1  [(another, Noun), (example, Noun), (sentence, ...   \n",
            "\n",
            "                    noun_phrases  \n",
            "0  [this is an example sentence]  \n",
            "1     [another example sentence]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Sentence completion with words or phrases using random prompts.\n",
        "import random\n",
        "\n",
        "# Dictionary with sentence prompts and possible completions\n",
        "sentence_prompts = {\n",
        "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
        "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
        "}\n",
        "\n",
        "input_prompt = \"After a long day at work, I like to relax by\"\n",
        "\n",
        "if input_prompt in sentence_prompts:\n",
        "    possible_completions = sentence_prompts[input_prompt]\n",
        "    print(\"Possible Completions:\")\n",
        "    for completion in possible_completions:\n",
        "        print(f\"- {input_prompt} {completion}\")\n",
        "else:\n",
        "    print(\"Prompt not found in the dictionary.\")\n",
        "\n",
        "# Use random to create a random sentence completion\n",
        "random_completion = random.choice([\"enjoying a cup of tea\", \"listening to music\", \"playing video games\"])\n",
        "print(f\"- {input_prompt} {random_completion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKFg-neKyKZF",
        "outputId": "9f839930-a19d-4013-cf1e-1450434881a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible Completions:\n",
            "- After a long day at work, I like to relax by watching my favorite TV show\n",
            "- After a long day at work, I like to relax by going for a walk\n",
            "- After a long day at work, I like to relax by reading a book\n",
            "- After a long day at work, I like to relax by enjoying a cup of tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Implement machine learning sentiment classification without using any pre defined NLP packages:\n",
        "'''Training Corpus:\n",
        "(\"I love this product\", \"positive\"),\n",
        "(\"This is excellent\", \"positive\"),\n",
        "(\"Terrible service\", \"negative\"),\n",
        "(\"It's okay, not great\", \"neutral\"),\n",
        "(\"Amazing experience\", \"positive\"),\n",
        "(\"Disappointing outcome\", \"negative\"),\n",
        "(\"Neutral feelings\", \"neutral\"),\n",
        "(\"I dislike it\", \"negative\")'''\n",
        "\n",
        "# Sample data\n",
        "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
        "\n",
        "# Training corpus\n",
        "training_corpus = [\n",
        "    (\"I love this product\", \"positive\"),\n",
        "    (\"This is excellent\", \"positive\"),\n",
        "    (\"Terrible service\", \"negative\"),\n",
        "    (\"It's okay, not great\", \"neutral\"),\n",
        "    (\"Amazing experience\", \"positive\"),\n",
        "    (\"Disappointing outcome\", \"negative\"),\n",
        "    (\"Neutral feelings\", \"neutral\"),\n",
        "    (\"I dislike it\", \"negative\")\n",
        "]\n",
        "\n",
        "# Function to determine sentiment label\n",
        "def determine_sentiment_label(text):\n",
        "    # Check using predefined keywords\n",
        "    if \"love\" in text.lower():\n",
        "        return 'positive'\n",
        "    elif \"terrible\" in text.lower():\n",
        "        return 'negative'\n",
        "    else:\n",
        "        # Check using the training corpus\n",
        "        for corpus_text, label in training_corpus:\n",
        "            if corpus_text.lower() in text.lower():\n",
        "                return label\n",
        "        # Default to neutral if no match found\n",
        "        return 'neutral'\n",
        "\n",
        "# Create result dictionary\n",
        "result_dict = {'text': data, 'label': [determine_sentiment_label(text) for text in data]}\n",
        "\n",
        "# Print results\n",
        "for text, label in zip(result_dict['text'], result_dict['label']):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Label: {label}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK5emKFiyqxN",
        "outputId": "43d84f9a-9ecf-4ef0-b5b2-57d8a6bdc8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product!\n",
            "Label: positive\n",
            "\n",
            "Text: It's terrible.\n",
            "Label: negative\n",
            "\n",
            "Text: Neutral statement.\n",
            "Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#or 7b.\n",
        "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
        "\n",
        "def determine_sentiment_label(text):\n",
        "    if \"love\" in text.lower():\n",
        "        return 'positive'\n",
        "    elif \"terrible\" in text.lower():\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "result_dict = {'text': data, 'label': [determine_sentiment_label(text) for text in data]}\n",
        "\n",
        "for text, label in zip(result_dict['text'], result_dict['label']):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Label: {label}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM0hpvyfzQGQ",
        "outputId": "67832240-eeb4-4c59-c176-6fba456a25ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love this product!\n",
            "Label: positive\n",
            "\n",
            "Text: It's terrible.\n",
            "Label: negative\n",
            "\n",
            "Text: Neutral statement.\n",
            "Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Text Summarization (Extractive and Abstractive)\n",
        "def simple_summarization(article, num_sentences=3):\n",
        "    sentences = article.split(\".\")\n",
        "    # Remove empty strings and strip leading/trailing whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Calculate the importance score for each sentence (based on sentence length)\n",
        "    scores = [len(sentence) for sentence in sentences]\n",
        "\n",
        "    # Select the top N sentences with the highest importance scores\n",
        "    selected_sentences = sorted(zip(sentences, scores), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
        "\n",
        "    # Extract the selected sentences\n",
        "    summary = [sentence for sentence, _ in selected_sentences]\n",
        "\n",
        "    # Join the selected sentences into a summary paragraph\n",
        "    return '. '.join(summary)\n",
        "\n",
        "# Example usage\n",
        "article_text = \"\"\"\n",
        "Sentiment analysis is increasingly vital in modern data-driven contexts, providing crucial insights into public opinion and emotional\n",
        " trends from vast textual datasets. This project aims to enhance sentiment analysis by addressing critical challenges such as interpreting\n",
        " figurative language, including sarcasm and irony, which often confound existing models. Furthermore, the project seeks to expand the scope\n",
        " of emotional analysis beyond traditional categories, aiming for a more nuanced understanding of human emotions. By integrating advanced\n",
        " natural language processing (NLP) techniques and machine learning algorithms, the project aims to develop a robust sentiment analysis\n",
        "  framework capable of delivering accurate insights across diverse platforms. Emphasis is placed on understanding platform-specific\n",
        "   sentiment dynamics to ensure consistent and relevant analyses across different social media and digital platforms. Ultimately,\n",
        "    these advancements are expected to empower businesses, marketers, and researchers with more reliable tools for making informed\n",
        "     decisions based on a deeper understanding of public sentiment and emotional trends.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a summary of the article\n",
        "summary = simple_summarization(article_text)\n",
        "print(\"Article Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BLyWQmzah8",
        "outputId": "c3aa4252-26fe-4494-a3c5-49b25cb374cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article Summary:\n",
            "By integrating advanced \n",
            " natural language processing (NLP) techniques and machine learning algorithms, the project aims to develop a robust sentiment analysis\n",
            "  framework capable of delivering accurate insights across diverse platforms. Ultimately,\n",
            "    these advancements are expected to empower businesses, marketers, and researchers with more reliable tools for making informed\n",
            "     decisions based on a deeper understanding of public sentiment and emotional trends. This project aims to enhance sentiment analysis by addressing critical challenges such as interpreting \n",
            " figurative language, including sarcasm and irony, which often confound existing models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Perform Name Entity Recognition (NER) on given corpus text.\n",
        "'''The capital of [France] is [Paris], a city known for its iconic [Eiffel Tower].\n",
        "[John Smith] visited [Tokyo] last summer, exploring the bustling streets of\n",
        "[Shibuya Crossing]. [May 5th, 2023] marks the anniversary of a significant\n",
        "event in [history]. [Elon Musk] is the CEO of [SpaceX] and [Tesla].'''\n",
        "import re\n",
        "\n",
        "# Corpus text\n",
        "corpus_text = \"The capital of [France] is [Paris], a city known for its iconic [Eiffel Tower]. [John Smith] visited [Tokyo] last summer, exploring the bustling streets of [Shibuya Crossing]. [May 5th, 2023] marks the anniversary of a significant event in [history]. [Elon Musk] is the CEO of [SpaceX] and [Tesla].\"\n",
        "\n",
        "# Initialize lists\n",
        "person_list = []\n",
        "place_list = []\n",
        "\n",
        "# Define regex pattern to find entities in brackets\n",
        "pattern = r'\\[(.*?)\\]'\n",
        "\n",
        "# Find all matches in the corpus text\n",
        "matches = re.findall(pattern, corpus_text)\n",
        "\n",
        "# Extract entities and populate lists\n",
        "for entity in matches:\n",
        "    if any(name in entity for name in [\"Barack Obama\", \"John Smith\", \"Elon Musk\"]):\n",
        "        person_list.append(entity)\n",
        "    elif any(place in entity for place in [\"Hawaii\", \"United States\", \"France\", \"Paris\", \"Tokyo\", \"Shibuya Crossing\"]):\n",
        "        place_list.append(entity)\n",
        "\n",
        "# Print the lists\n",
        "print(\"Person List:\", person_list)\n",
        "print(\"Place List:\", place_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9veiGPRCz34i",
        "outputId": "35a5c7a1-2164-4ff0-8c8f-c9cd825eeadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person List: ['John Smith', 'Elon Musk']\n",
            "Place List: ['France', 'Paris', 'Tokyo', 'Shibuya Crossing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Perform Morphological analysis without using any pre defined NLP packages\n",
        "'''The text corpus given below :\n",
        "The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with\n",
        "a ball. Running swiftly, the athlete won the race. The painted houses lined the\n",
        "street, attracting curious onlookers.'''\n",
        "\n",
        "# Tokenizer function\n",
        "def simple_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "# Stemming function\n",
        "def simple_porter_stemmer(word):\n",
        "    # A simple stemming function (for illustration purposes)\n",
        "    if word.endswith(\"es\"):\n",
        "        return word[:-2]\n",
        "    elif word.endswith(\"s\"):\n",
        "        return word[:-1]\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return word[:-3]\n",
        "    return word\n",
        "\n",
        "# Lemmatization function\n",
        "def simple_wordnet_lemmatizer(word):\n",
        "    # A simple lemmatization function (for illustration purposes)\n",
        "    if word.endswith(\"es\"):\n",
        "        return word[:-2]\n",
        "    elif word.endswith(\"s\"):\n",
        "        return word[:-1]\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return word[:-3]\n",
        "    return word\n",
        "\n",
        "# Function to analyze morphemes\n",
        "def analyze_morphemes(word, prefixes, root, suffixes):\n",
        "    morphemes = []\n",
        "    for prefix in prefixes:\n",
        "        if word.startswith(prefix):\n",
        "            morphemes.append(prefix)\n",
        "            word = word[len(prefix):]\n",
        "    morphemes.append(root)\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            morphemes.append(suffix)\n",
        "            word = word[:-len(suffix)]\n",
        "    return morphemes\n",
        "\n",
        "# Text corpus\n",
        "text = \"\"\"The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with\n",
        "a ball. Running swiftly, the athlete won the race. The painted houses lined the\n",
        "street, attracting curious onlookers.\"\"\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = simple_tokenizer(text)\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "stemmed_words = [simple_porter_stemmer(word) for word in words]\n",
        "lemmatized_words = [simple_wordnet_lemmatizer(word) for word in words]\n",
        "\n",
        "# Print original, stemmed, and lemmatized words\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n",
        "print()\n",
        "\n",
        "# Morpheme analysis example\n",
        "word = \"misunderstanding\"\n",
        "prefixes = [\"mis\"]\n",
        "root = \"understand\"\n",
        "suffixes = [\"ing\"]\n",
        "morphemes = analyze_morphemes(word, prefixes, root, suffixes)\n",
        "\n",
        "# Print morphemes\n",
        "print(\"Word:\", word)\n",
        "print(\"Morphemes:\", morphemes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGZMfOzI1A8N",
        "outputId": "01774046-ca9c-4b8d-fddf-88508035278f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'quick', 'brown', 'foxes', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary's\", 'cat', 'is', 'playing', 'with', 'a', 'ball.', 'Running', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'houses', 'lined', 'the', 'street,', 'attracting', 'curious', 'onlookers.']\n",
            "Stemmed words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
            "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
            "\n",
            "Word: misunderstanding\n",
            "Morphemes: ['mis', 'understand', 'ing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XPtntfme1s8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}