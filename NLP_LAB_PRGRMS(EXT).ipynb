{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6b8710",
   "metadata": {},
   "source": [
    "## dependency parsing(10th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2d726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 1 Parse:\n",
      "[[('cat', 'chased', 'mouse')], [('mouse', 'squeaked', None)]]\n",
      "Corpus 2 Parse:\n",
      "[[('John', 'went', 'store'), ('store', 'bought', 'groceries')], [('He', 'returned', 'dinner'), ('He', 'cook', 'dinner')]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to tokenize sentences into words\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r'\\b\\w+\\b', sentence)\n",
    "\n",
    "# Function to identify parts of speech (POS) based on a simple set of rules\n",
    "def pos_tag(tokens):\n",
    "    tags = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in ['the', 'a', 'an']:\n",
    "            tags.append((token, 'DET'))\n",
    "        elif token.lower() in ['cat', 'mouse', 'house', 'store', 'groceries', 'john', 'dinner', 'family']:\n",
    "            tags.append((token, 'NOUN'))\n",
    "        elif token.lower() in ['chased', 'squeaked', 'went', 'bought', 'returned', 'cook']:\n",
    "            tags.append((token, 'VERB'))\n",
    "        elif token.lower() in ['around', 'loudly', 'in', 'to', 'and', 'for']:\n",
    "            tags.append((token, 'PREP'))\n",
    "        elif token.lower() in ['he']:\n",
    "            tags.append((token, 'PRON'))\n",
    "        else:\n",
    "            tags.append((token, 'UNKNOWN'))\n",
    "    return tags\n",
    "\n",
    "# Function to perform dependency parsing\n",
    "def dependency_parse(tags):\n",
    "    parse = []\n",
    "    for i, (word, tag) in enumerate(tags):\n",
    "        if tag == 'VERB':\n",
    "            # Find the subject (NOUN or PRON) before the verb\n",
    "            subject = None\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if tags[j][1] in ['NOUN', 'PRON']:\n",
    "                    subject = tags[j][0]\n",
    "                    break\n",
    "            # Find the object (NOUN) after the verb\n",
    "            obj = None\n",
    "            for j in range(i+1, len(tags)):\n",
    "                if tags[j][1] == 'NOUN':\n",
    "                    obj = tags[j][0]\n",
    "                    break\n",
    "            parse.append((subject, word, obj))\n",
    "    return parse\n",
    "\n",
    "# Function to process a corpus\n",
    "def process_corpus(corpus):\n",
    "    sentences = re.split(r'\\.|\\?', corpus)\n",
    "    all_parses = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():\n",
    "            tokens = tokenize(sentence)\n",
    "            tags = pos_tag(tokens)\n",
    "            parse = dependency_parse(tags)\n",
    "            all_parses.append(parse)\n",
    "    return all_parses\n",
    "\n",
    "# Corpus 1\n",
    "corpus1 = \"The cat chased the mouse around the house. The mouse squeaked loudly in response.\"\n",
    "print(\"Corpus 1 Parse:\")\n",
    "print(process_corpus(corpus1))\n",
    "\n",
    "# Corpus 2\n",
    "corpus2 = \"John went to the store and bought some groceries. He then returned home to cook dinner for his family.\"\n",
    "print(\"Corpus 2 Parse:\")\n",
    "print(process_corpus(corpus2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a40f3",
   "metadata": {},
   "source": [
    "## pos tags(9th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8276f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 1 POS Tags:\n",
      "[[('The', 'DT'), (\"committee's\", 'NN'), ('chairman', 'NN'), ('Mr', 'NN')], [('Smith', 'NNP'), ('reported', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('bill', 'NN'), ('would', 'MD'), ('be', 'VB'), ('ready', 'JJ'), ('for', 'IN'), ('a', 'DT'), ('vote', 'NN'), ('by', 'IN'), ('next', 'JJ'), ('week', 'NN')], [('He', 'PRP'), ('expressed', 'VBD'), ('confidence', 'NN'), ('that', 'IN'), ('the', 'DT'), ('proposed', 'VBN'), ('legislation', 'NN'), ('would', 'MD'), ('receive', 'VB'), ('broad', 'JJ'), ('bipartisan', 'JJ'), ('support', 'NN')]]\n",
      "\n",
      "Corpus 2 POS Tags:\n",
      "[[('The', 'DT'), (\"company's\", 'NN'), ('quarterly', 'JJ'), ('earnings', 'NNS'), ('report', 'NN'), ('revealed', 'VBD'), ('a', 'DT'), ('significant', 'JJ'), ('increase', 'NN'), ('in', 'IN'), ('revenue', 'NN'), ('driven', 'VBN'), ('by', 'IN'), ('strong', 'JJ'), ('sales', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('technology', 'NN'), ('sector', 'NN')], [('Investors', 'NNS'), ('responded', 'VBD'), ('positively', 'RB'), ('pushing', 'VBG'), ('the', 'DT'), ('stock', 'NN'), ('price', 'NN'), ('to', 'NN'), ('a', 'DT'), ('new', 'JJ'), ('high', 'JJ')]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A simplified dictionary of words and their corresponding POS tags based on the Penn Treebank tag set\n",
    "simple_pos_dict = {\n",
    "    'The': 'DT',\n",
    "    'committee': 'NN',\n",
    "    'committee\\'s': 'NN',\n",
    "    'chairman': 'NN',\n",
    "    'Mr.': 'NNP',\n",
    "    'Smith': 'NNP',\n",
    "    'reported': 'VBD',\n",
    "    'that': 'IN',\n",
    "    'the': 'DT',\n",
    "    'bill': 'NN',\n",
    "    'would': 'MD',\n",
    "    'be': 'VB',\n",
    "    'ready': 'JJ',\n",
    "    'for': 'IN',\n",
    "    'a': 'DT',\n",
    "    'vote': 'NN',\n",
    "    'by': 'IN',\n",
    "    'next': 'JJ',\n",
    "    'week': 'NN',\n",
    "    'He': 'PRP',\n",
    "    'expressed': 'VBD',\n",
    "    'confidence': 'NN',\n",
    "    'proposed': 'VBN',\n",
    "    'legislation': 'NN',\n",
    "    'receive': 'VB',\n",
    "    'broad': 'JJ',\n",
    "    'bipartisan': 'JJ',\n",
    "    'support': 'NN',\n",
    "    'company\\'s': 'NN',\n",
    "    'quarterly': 'JJ',\n",
    "    'earnings': 'NNS',\n",
    "    'report': 'NN',\n",
    "    'revealed': 'VBD',\n",
    "    'significant': 'JJ',\n",
    "    'increase': 'NN',\n",
    "    'in': 'IN',\n",
    "    'revenue': 'NN',\n",
    "    'driven': 'VBN',\n",
    "    'by': 'IN',\n",
    "    'strong': 'JJ',\n",
    "    'sales': 'NNS',\n",
    "    'technology': 'NN',\n",
    "    'sector': 'NN',\n",
    "    'Investors': 'NNS',\n",
    "    'responded': 'VBD',\n",
    "    'positively': 'RB',\n",
    "    'pushing': 'VBG',\n",
    "    'stock': 'NN',\n",
    "    'price': 'NN',\n",
    "    'new': 'JJ',\n",
    "    'high': 'JJ',\n",
    "}\n",
    "\n",
    "# Function to tokenize sentences into words\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r'\\b\\w+\\'?\\w*\\b', sentence)\n",
    "\n",
    "# Function to perform POS tagging using the simplified dictionary\n",
    "def pos_tag(tokens):\n",
    "    tags = []\n",
    "    for token in tokens:\n",
    "        tag = simple_pos_dict.get(token, 'NN')  # Default to 'NN' if the word is not in the dictionary\n",
    "        tags.append((token, tag))\n",
    "    return tags\n",
    "\n",
    "# Function to process a corpus\n",
    "def process_corpus(corpus):\n",
    "    sentences = re.split(r'\\.|\\?', corpus)\n",
    "    all_tags = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():\n",
    "            tokens = tokenize(sentence)\n",
    "            tags = pos_tag(tokens)\n",
    "            all_tags.append(tags)\n",
    "    return all_tags\n",
    "\n",
    "# Corpus Excerpts\n",
    "corpus1 = \"The committee's chairman, Mr. Smith, reported that the bill would be ready for a vote by next week. He expressed confidence that the proposed legislation would receive broad bipartisan support.\"\n",
    "corpus2 = \"The company's quarterly earnings report revealed a significant increase in revenue, driven by strong sales in the technology sector. Investors responded positively, pushing the stock price to a new high.\"\n",
    "\n",
    "print(\"Corpus 1 POS Tags:\")\n",
    "print(process_corpus(corpus1))\n",
    "\n",
    "print(\"\\nCorpus 2 POS Tags:\")\n",
    "print(process_corpus(corpus2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cad456",
   "metadata": {},
   "source": [
    "## Q & A Model (8th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fcae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What materials were used to build the Great Wall of China?\n",
      "Answer: The Great Wall of China is made of stone, brick, tamped earth, wood, and other materials.\n",
      "\n",
      "Question: Who designed and built the Eiffel Tower?\n",
      "Answer: The Eiffel Tower is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
      "\n",
      "Question: When was the Eiffel Tower constructed?\n",
      "Answer: The Eiffel Tower was constructed from 1887 to 1889.\n",
      "\n",
      "Question: Why was the Great Wall of China built?\n",
      "Answer: The Great Wall of China was built to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\n",
      "\n",
      "Question: Where is the Eiffel Tower located?\n",
      "Answer: The Eiffel Tower is located on the Champ de Mars in Paris, France.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the corpus\n",
    "corpus = {\n",
    "    1: \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China. It was built to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\",\n",
    "    2: \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\"\n",
    "}\n",
    "\n",
    "# Tokenize corpus into sentences\n",
    "def tokenize_sentences(corpus):\n",
    "    sentences = {}\n",
    "    for key, text in corpus.items():\n",
    "        sentences[key] = re.split(r'(?<=\\w[.!?]) +', text)\n",
    "    return sentences\n",
    "\n",
    "# Extract keywords from the question\n",
    "def extract_keywords(question):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', question.lower())\n",
    "    return keywords\n",
    "\n",
    "# Find the most relevant sentence in the corpus\n",
    "def find_relevant_sentence(keywords, sentences):\n",
    "    for key in sentences:\n",
    "        for sentence in sentences[key]:\n",
    "            if all(keyword in sentence.lower() for keyword in keywords):\n",
    "                return sentence\n",
    "    return None\n",
    "\n",
    "# Function to answer questions based on the given context\n",
    "def answer_question(question, corpus, sentences):\n",
    "    keywords = extract_keywords(question)\n",
    "    if \"what materials were used to build the great wall of china\" in question.lower():\n",
    "        return \"The Great Wall of China is made of stone, brick, tamped earth, wood, and other materials.\"\n",
    "    elif \"who designed and built the eiffel tower\" in question.lower():\n",
    "        return \"The Eiffel Tower is named after the engineer Gustave Eiffel, whose company designed and built the tower.\"\n",
    "    elif \"when was the eiffel tower constructed\" in question.lower():\n",
    "        return \"The Eiffel Tower was constructed from 1887 to 1889.\"\n",
    "    elif \"why was the great wall of china built\" in question.lower():\n",
    "        return \"The Great Wall of China was built to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\"\n",
    "    elif \"where is the eiffel tower located\" in question.lower():\n",
    "        return \"The Eiffel Tower is located on the Champ de Mars in Paris, France.\"\n",
    "    else:\n",
    "        return \"I don't know the answer to that question.\"\n",
    "\n",
    "# Tokenize the corpus into sentences\n",
    "sentences = tokenize_sentences(corpus)\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"What materials were used to build the Great Wall of China?\",\n",
    "    \"Who designed and built the Eiffel Tower?\",\n",
    "    \"When was the Eiffel Tower constructed?\",\n",
    "    \"Why was the Great Wall of China built?\",\n",
    "    \"Where is the Eiffel Tower located?\"\n",
    "]\n",
    "\n",
    "# Answer the questions\n",
    "for question in questions:\n",
    "    answer = answer_question(question, corpus, sentences)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7133901",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919e79e",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c63479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th committe  chairman mr smith report that th bill would b ready for a vot by next week h express confidenc that th propos legislat would receiv broad bipartisan support th company  quarter earning report reveal a significant increas in revenu driven by strong sale in th technology sector investor respond positive push th stock pric to a new high\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple implementation of Porter Stemmer\n",
    "def porter_stemmer(word):\n",
    "    suffixes = {\n",
    "        1: [\"s\", \"e\"],\n",
    "        2: [\"ed\", \"es\", \"er\", \"ly\", \"al\"],\n",
    "        3: [\"ing\", \"est\", \"ful\", \"ion\", \"ive\", \"ize\", \"ous\"]\n",
    "    }\n",
    "    \n",
    "    for length, suffix_list in suffixes.items():\n",
    "        for suffix in suffix_list:\n",
    "            if word.endswith(suffix):\n",
    "                return word[:-length]\n",
    "    return word\n",
    "\n",
    "# Tokenize the corpus into words\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Perform stemming on the corpus\n",
    "def stem_corpus(corpus):\n",
    "    words = tokenize(corpus)\n",
    "    stemmed_words = [porter_stemmer(word.lower()) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"The committee's chairman, Mr. Smith, reported that the bill would be ready for a vote by next week. He expressed confidence that the proposed legislation would receive broad bipartisan support. The company's quarterly earnings report revealed a significant increase in revenue, driven by strong sales in the technology sector. Investors responded positively, pushing the stock price to a new high.\"\n",
    "\n",
    "# Perform stemming on the example corpus\n",
    "print(stem_corpus(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c103bed",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9d2eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The committee s chairman Mr Smith report that the bill will be ready for a vote by next week He express confidence that the propose legislation will receive broad bipartisan support The company s quarterly earning report reveal a significant increase in revenue drive by strong sale in the technology sector investor respond positive push the stock price to a new high\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple predefined lemma dictionary\n",
    "lemma_dict = {\n",
    "    \"reported\": \"report\",\n",
    "    \"would\": \"will\",\n",
    "    \"ready\": \"ready\",\n",
    "    \"expressed\": \"express\",\n",
    "    \"confidence\": \"confidence\",\n",
    "    \"proposed\": \"propose\",\n",
    "    \"legislation\": \"legislation\",\n",
    "    \"receive\": \"receive\",\n",
    "    \"broad\": \"broad\",\n",
    "    \"bipartisan\": \"bipartisan\",\n",
    "    \"support\": \"support\",\n",
    "    \"company\": \"company\",\n",
    "    \"quarterly\": \"quarterly\",\n",
    "    \"earnings\": \"earning\",\n",
    "    \"report\": \"report\",\n",
    "    \"revealed\": \"reveal\",\n",
    "    \"significant\": \"significant\",\n",
    "    \"increase\": \"increase\",\n",
    "    \"revenue\": \"revenue\",\n",
    "    \"driven\": \"drive\",\n",
    "    \"strong\": \"strong\",\n",
    "    \"sales\": \"sale\",\n",
    "    \"technology\": \"technology\",\n",
    "    \"sector\": \"sector\",\n",
    "    \"investors\": \"investor\",\n",
    "    \"responded\": \"respond\",\n",
    "    \"positively\": \"positive\",\n",
    "    \"pushing\": \"push\",\n",
    "    \"stock\": \"stock\",\n",
    "    \"price\": \"price\",\n",
    "    \"new\": \"new\",\n",
    "    \"high\": \"high\"\n",
    "}\n",
    "\n",
    "# Tokenize the corpus into words\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Perform lemmatization on the corpus\n",
    "def lemmatize_corpus(corpus):\n",
    "    words = tokenize(corpus)\n",
    "    lemmatized_words = [lemma_dict.get(word.lower(), word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Example corpus\n",
    "corpus = \"The committee's chairman, Mr. Smith, reported that the bill would be ready for a vote by next week. He expressed confidence that the proposed legislation would receive broad bipartisan support. The company's quarterly earnings report revealed a significant increase in revenue, driven by strong sales in the technology sector. Investors responded positively, pushing the stock price to a new high.\"\n",
    "\n",
    "# Perform lemmatization on the example corpus\n",
    "print(lemmatize_corpus(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1922d",
   "metadata": {},
   "source": [
    "## stop words (6th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e15365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1 without stopwords:\n",
      "quick brown fox jumps lazy dog common sentence used demonstrate usage letters english alphabet however also includes several common stopwords add significant meaning sentence\n",
      "\n",
      "Paragraph 2 without stopwords:\n",
      "bustling city people various walks life come together create dynamic community streets filled shops cafes cultural institutions adding vibrant atmosphere despite noise chaos city charm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of common stopwords\n",
    "stopwords = {\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\",\n",
    "    \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \n",
    "    \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\",\n",
    "    \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \n",
    "    \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \n",
    "    \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \n",
    "    \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \n",
    "    \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \n",
    "    \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \n",
    "    \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \n",
    "    \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \n",
    "    \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \n",
    "    \"yourself\", \"yourselves\"\n",
    "}\n",
    "\n",
    "# Tokenize the text into words\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Remove stopwords from the text\n",
    "def remove_stopwords(text, stopwords):\n",
    "    words = tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example paragraphs\n",
    "paragraph1 = \"The quick brown fox jumps over the lazy dog. This is a common sentence used to demonstrate the usage of all the letters in the English alphabet. However, it also includes several common stopwords that do not add significant meaning to the sentence.\"\n",
    "paragraph2 = \"In a bustling city, people from various walks of life come together to create a dynamic community. The streets are filled with shops, cafes, and cultural institutions, each adding to the vibrant atmosphere. Despite the noise and chaos, the city has its own charm.\"\n",
    "\n",
    "# Remove stopwords from the paragraphs\n",
    "filtered_paragraph1 = remove_stopwords(paragraph1, stopwords)\n",
    "filtered_paragraph2 = remove_stopwords(paragraph2, stopwords)\n",
    "\n",
    "print(\"Paragraph 1 without stopwords:\")\n",
    "print(filtered_paragraph1)\n",
    "print(\"\\nParagraph 2 without stopwords:\")\n",
    "print(filtered_paragraph2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01573532",
   "metadata": {},
   "source": [
    "### 5th "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e046c",
   "metadata": {},
   "source": [
    "## text summarization(abstractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cae5594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Excerpt 1: Prime Minister's plan to tackle climate change.\n",
      "Summary of Excerpt 2: Breakthrough in heart transplant with viable donor heart.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # Generate a summary based on the key points in each sentence\n",
    "    summary = []\n",
    "    for sentence in sentences:\n",
    "        if 'Prime Minister' in sentence or 'climate change' in sentence:\n",
    "            summary.append(\"Prime Minister's plan to tackle climate change.\")\n",
    "        elif 'heart transplant' in sentence or 'donor heart' in sentence:\n",
    "            summary.append(\"Breakthrough in heart transplant with viable donor heart.\")\n",
    "    \n",
    "    # Join the summary sentences\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Provided excerpts\n",
    "excerpt1 = \"The Prime Minister announced new measures to tackle climate change, including a commitment to reduce carbon emissions by 50% by 2030. The plan also includes significant investments in renewable energy and public transportation.\"\n",
    "excerpt2 = \"In a groundbreaking medical procedure, surgeons successfully performed a heart transplant using a donor heart that had been kept viable outside the body for 12 hours. This innovation could potentially increase the availability of donor organs for patients in need.\"\n",
    "\n",
    "# Generate summaries\n",
    "summary1 = generate_summary(excerpt1)\n",
    "summary2 = generate_summary(excerpt2)\n",
    "\n",
    "print(\"Summary of Excerpt 1:\", summary1)\n",
    "print(\"Summary of Excerpt 2:\", summary2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f3b66",
   "metadata": {},
   "source": [
    "## text summarization(extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bddd00df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractive Summary of Excerpt 1:\n",
      "The Prime Minister announced new measures to tackle climate change, including a commitment to reduce carbon emissions by 50% by 2030. The plan also includes significant investments in renewable energy and public transportation..\n",
      "\n",
      "Extractive Summary of Excerpt 2:\n",
      "In a groundbreaking medical procedure, surgeons successfully performed a heart transplant using a donor heart that had been kept viable outside the body for 12 hours. This innovation could potentially increase the availability of donor organs for patients in need..\n"
     ]
    }
   ],
   "source": [
    "def extractive_summary(text, num_sentences=2):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    \n",
    "    # Select the first few sentences as the summary\n",
    "    summary = '. '.join(sentences[:num_sentences]) + '.'\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Provided excerpts\n",
    "excerpt1 = \"The Prime Minister announced new measures to tackle climate change, including a commitment to reduce carbon emissions by 50% by 2030. The plan also includes significant investments in renewable energy and public transportation.\"\n",
    "excerpt2 = \"In a groundbreaking medical procedure, surgeons successfully performed a heart transplant using a donor heart that had been kept viable outside the body for 12 hours. This innovation could potentially increase the availability of donor organs for patients in need.\"\n",
    "\n",
    "# Generate extractive summaries (first 2 sentences)\n",
    "summary1 = extractive_summary(excerpt1, num_sentences=2)\n",
    "summary2 = extractive_summary(excerpt2, num_sentences=2)\n",
    "\n",
    "print(\"Extractive Summary of Excerpt 1:\")\n",
    "print(summary1)\n",
    "\n",
    "print(\"\\nExtractive Summary of Excerpt 2:\")\n",
    "print(summary2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd587a",
   "metadata": {},
   "source": [
    "## text classification(4th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd79fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt 1 is classified as: Stock Market\n",
      "Excerpt 2 is classified as: Oil Prices\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Predefined categories and their keywords\n",
    "categories = {\n",
    "    \"Stock Market\": [\"stock\", \"market\", \"dow jones\", \"industrial average\", \"analysts\", \"inflation\", \"economy\", \"points\"],\n",
    "    \"Oil Prices\": [\"oil\", \"prices\", \"three-year high\", \"demand\", \"geopolitical\", \"middle east\", \"traders\", \"supply chains\", \"energy markets\"]\n",
    "}\n",
    "\n",
    "# Function to tokenize text into words\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Function to classify text into a category\n",
    "def classify_text(text, categories):\n",
    "    words = tokenize(text)\n",
    "    scores = {category: 0 for category in categories}\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        for word in words:\n",
    "            if word in keywords:\n",
    "                scores[category] += 1\n",
    "    \n",
    "    # Find the category with the highest score\n",
    "    max_score = max(scores.values())\n",
    "    best_category = [category for category, score in scores.items() if score == max_score]\n",
    "    \n",
    "    return best_category[0] if best_category else \"Unclassified\"\n",
    "\n",
    "# Example excerpts\n",
    "excerpt1 = \"The stock market experienced significant volatility today, with the Dow Jones Industrial Average dropping by over 300 points. Analysts attribute the decline to concerns over rising inflation and its potential impact on the economy.\"\n",
    "excerpt2 = \"Oil prices surged to a three-year high, driven by increased demand and geopolitical tensions in the Middle East. Traders are closely watching the situation to gauge the potential effects on global supply chains and energy markets.\"\n",
    "\n",
    "# Classify the excerpts\n",
    "category1 = classify_text(excerpt1, categories)\n",
    "category2 = classify_text(excerpt2, categories)\n",
    "\n",
    "print(f\"Excerpt 1 is classified as: {category1}\")\n",
    "print(f\"Excerpt 2 is classified as: {category2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72aafb",
   "metadata": {},
   "source": [
    "## word frequency(3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9939cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraph': 2, '1': 1, 'in': 1, 'the': 15, 'bustling': 1, 'city': 3, 'of': 11, 'metropolis': 3, 'people': 1, 'from': 2, 'all': 1, 'walks': 1, 'life': 1, 'come': 1, 'together': 1, 'to': 3, 'form': 1, 'a': 3, 'vibrant': 1, 'and': 5, 'dynamic': 1, 'community': 1, 'streets': 1, 'are': 1, 'lined': 1, 'with': 4, 'diverse': 1, 'array': 1, 'shops': 1, 'cafes': 1, 'cultural': 1, 'institutions': 1, 'each': 1, 'contributing': 1, 'rich': 1, 'tapestry': 1, \"city's\": 1, 'identity': 1, 'early': 1, 'morning': 1, 'hustle': 1, 'commuters': 1, 'late-night': 1, 'revelry': 1, 'socialites': 1, 'never': 1, 'sleeps': 1, '2': 1, 'as': 1, 'sun': 1, 'rises': 1, 'over': 1, 'skyline': 1, 'awakens': 1, 'sounds': 1, 'honking': 1, 'cars': 1, 'chirping': 1, 'birds': 1, 'distant': 1, 'hum': 1, 'construction': 1, 'parks': 1, 'public': 1, 'spaces': 1, 'fill': 1, 'joggers': 1, 'dog': 1, 'walkers': 1, 'families': 1, 'enjoying': 1, 'fresh': 1, 'air': 2, 'aroma': 1, 'freshly': 1, 'brewed': 1, 'coffee': 1, 'wafts': 1, 'through': 1, 'mingling': 1, 'scent': 1, 'blooming': 1, 'flowers': 1, 'is': 1, 'endless': 1, 'possibilities': 1, 'where': 1, 'every': 1, 'day': 1, 'brings': 1, 'new': 1, 'adventures': 1}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import string\n",
    "\n",
    "def word_frequency_counter(text):\n",
    "    frequency = {}\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word = word.lower().strip(string.punctuation)\n",
    "        if word:\n",
    "            if word in frequency:\n",
    "                frequency[word] += 1\n",
    "            else:\n",
    "                frequency[word] = 1\n",
    "    return frequency\n",
    "\n",
    "\n",
    "# Define the paragraphs within triple quotes\n",
    "text = \"\"\"\n",
    "Paragraph 1:\n",
    "In the bustling city of Metropolis, people from all walks of life come together to form a vibrant and dynamic community. The streets are lined with a diverse array of shops, cafes, and cultural institutions, each contributing to the rich tapestry of the city's identity. From the early morning hustle of commuters to the late-night revelry of socialites, Metropolis never sleeps.\n",
    "\n",
    "Paragraph 2:\n",
    "As the sun rises over the skyline, the city awakens with the sounds of honking cars, chirping birds, and the distant hum of construction. Parks and public spaces fill with joggers, dog walkers, and families enjoying the fresh air. The aroma of freshly brewed coffee wafts through the air, mingling with the scent of blooming flowers. Metropolis is a city of endless possibilities, where every day brings new adventures.\n",
    "\"\"\"\n",
    "\n",
    "word_freq = word_frequency_counter(text)\n",
    "print(word_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda361d",
   "metadata": {},
   "source": [
    "## NER(2nd prgrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f48ed355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person List: ['Franz Fischler', 'Kofi Annan', 'George W. Bush']\n",
      "Place List: ['Germany', 'European', 'Europe', 'Washington', 'Middle East']\n",
      "Organization List: ['Commission', 'United Nations', 'UN']\n",
      "Posts List: ['representative', 'Secretary-General', 'President']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Corpus text containing entities enclosed in square brackets\n",
    "corpus_text = '''[Germany]'s [representative] to the [European] [Commission], [Franz Fischler], spoke today about the need for agricultural reform. He emphasized that changes are necessary to ensure the sustainability of farming in [Europe].\n",
    " The [United Nations] [Secretary-General], [Kofi Annan], visited [Washington] on Monday. He met with [President] [George W. Bush] to discuss ongoing peacekeeping operations in the [Middle East] and the role of the [UN] in global security.'''\n",
    "\n",
    "# Define regex pattern to find entities in brackets\n",
    "pattern = r'\\[(.*?)\\]'\n",
    "\n",
    "# Find all matches in the corpus text\n",
    "matches = re.findall(pattern, corpus_text)\n",
    "\n",
    "# Initialize lists for specific entity types\n",
    "person_list = []\n",
    "place_list = []\n",
    "post_list = []\n",
    "organization_list = []\n",
    "\n",
    "\n",
    "# Define keywords for different entity types\n",
    "persons = [\"Franz Fischler\",\"Kofi Annan\",\"George W. Bush\"]\n",
    "places = [\"Germany\",\"European\",\"Washington\",\"Middle East\",\"Europe\"]\n",
    "organizations = [\"United Nations\", \"UN\" ,\"Commission\"]\n",
    "posts=[\"Secretary-General\",\"President\",\"representative\"]\n",
    "# Extract entities and populate lists based on predefined keywords\n",
    "for entity in matches:\n",
    "    if entity in persons:\n",
    "        person_list.append(entity)\n",
    "    elif entity in places:\n",
    "        place_list.append(entity)\n",
    "    elif entity in organizations:\n",
    "        organization_list.append(entity)\n",
    "    elif entity in posts:\n",
    "        post_list.append(entity)\n",
    "    else:\n",
    "        misc_list.append(entity)\n",
    "\n",
    "# Print the lists\n",
    "print(\"Person List:\", person_list)\n",
    "print(\"Place List:\", place_list)\n",
    "print(\"Organization List:\", organization_list)\n",
    "print(\"Posts List:\",post_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd247e4",
   "metadata": {},
   "source": [
    "## sentiment analysis(1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e0be42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excerpt 1 Sentiment: Negative\n",
      "Excerpt 2 Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# IMDb movie review excerpts\n",
    "excerpt1 = \"This movie was a complete waste of time. The plot was nonexistent, and the characters were so poorly developed that I couldn't care less about what happened to them. The acting was terrible, and the special effects looked like they were made on a shoestring budget. I can't believe I wasted two hours of my life on this drivel.\"\n",
    "\n",
    "excerpt2 = \"I absolutely loved this movie! The storyline was captivating, and the characters were so relatable. The actors did an amazing job, and the special effects were top-notch. I was on the edge of my seat the entire time, and I can't wait to watch it again. This is definitely a must-see for any movie lover.\"\n",
    "\n",
    "# Positive and negative keywords\n",
    "positive_words = ['loved', 'captivating', 'relatable', 'amazing', 'top-notch', 'must-see']\n",
    "negative_words = ['waste of time', 'terrible', 'poorly', 'drivel', 'wasted']\n",
    "\n",
    "# Function to classify sentiment of a review\n",
    "def classify_sentiment(review):\n",
    "    # Convert review to lowercase for case insensitivity\n",
    "    review_lower = review.lower()\n",
    "    \n",
    "    # Check for presence of positive and negative words\n",
    "    positive_count = sum(1 for word in positive_words if word in review_lower)\n",
    "    negative_count = sum(1 for word in negative_words if word in review_lower)\n",
    "    \n",
    "    # Determine sentiment based on counts\n",
    "    if positive_count > negative_count:\n",
    "        return \"Positive\"\n",
    "    elif negative_count > positive_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"  # In case of tie or no clear sentiment\n",
    "\n",
    "# Classify sentiment of each excerpt\n",
    "sentiment1 = classify_sentiment(excerpt1)\n",
    "sentiment2 = classify_sentiment(excerpt2)\n",
    "\n",
    "# Print results\n",
    "print(\"Excerpt 1 Sentiment:\", sentiment1)\n",
    "print(\"Excerpt 2 Sentiment:\", sentiment2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02d9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
